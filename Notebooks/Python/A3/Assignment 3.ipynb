{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "## COGS 189: Brain Computer Interfaces\n",
    "## Winter 23, Instructor: Alessandro D'Amico\n",
    "### Due Date: 05Feb2023\n",
    "***\n",
    "## Overview:\n",
    "For this assignment (tutorial), we will be working with a BCI competition dataset, with the primary goal of classifying target vs non-target trials from a P300 speller. The data we will be using comes from the 2nd Berlin BCI Competition, and was provided by Wadsworth Center, NYS Department of Health (Jonathan R. Wolpaw). Please go to the following link: \n",
    "http://www.bbci.de/competition/ii/\n",
    "<br><br>\n",
    "We will be working with **Dataset IIb**. Please click on the description of this dataset (pdf format is superior) and read through the page in order to answer the following questions.\n",
    "\n",
    "Here is a direct link: http://www.bbci.de/competition/ii/albany_desc/albany_desc_ii.pdf\n",
    "***\n",
    "***Question 1:*** What is the *dt* (in ms; round to the nearest ms) for the row/column intensification? \n",
    "<br>*(Hint: dt is the time between samples. For example, the dt at 1000 Hz = 1ms, 500 Hz = 2ms, 200 Hz = 5ms)*<br>\n",
    "***Question 2:*** How long (in ms) was each row/column intensified?<br>\n",
    "***Question 3:*** How much time (in ms) was there between each row/column intensification?<br>\n",
    "***Question 4:*** What is the sampling rate of the EEG data? **(DO NOT INCLUDE Hz IN YOUR ANSWER)**<br>\n",
    "***Question 5:*** Within a single block of 12 trials, how many times is each character illuminated?\n",
    "<br>*(Hint: A single block of 12 trials consists of each of the 6 rows being illuminated and each of the 6 columns being illuminated)*\n",
    "***\n",
    "<img src=\"assets/P300Matrix.gif\">\n",
    "\n",
    "*I created the animation following the description of the dataset, although this was not what participants actually saw. As you can tell, these stimuli are being presented in rapid succession. The participant was instructed to only focus on one character at a time. This shows 15 blocks of 12 intensifications which was the number of trials per letter described in the description*\n",
    "***\n",
    "***Note:*** The description of this dataset does not detail what the reference electrode was; this is bad practice. It took quite a long time to find, but I believe the reference used was the right earlobe as described in this paper:<br> https://hal.archives-ouvertes.fr/file/index/docid/521054/filename/krusienski_jne_2006.pdf\n",
    "<br>\n",
    "This is the same group and the parameters for data collection seem identical which is why I believe the descriptions of that paper apply to the data we'll be working with. They also mention they have already filtered their data with a 0.1 - 60 Hz bandpass filter. We will still apply a bandpass filter with a 0.1 Hz low cut off.\n",
    "***\n",
    "The owners of this data provided us with useful descriptions of the data, along with some matlab code. We will use their documentation to epoch the data so that we can train our classifier. We will be using linear discriminant analysis (LDA) for this problem, as it performs very well on P300 paradigms. Furthermore, this algorithm is rather intuitive. We will be using ```sklearn``` as it will make this assignment easier to follow.\n",
    "\n",
    "## Section 1: Setup\n",
    "Before we can begin processing our data, we must first import some useful packages. <br>\n",
    "- `numpy` is used widely for processing numerical data, and supports matrix operations natively. We will be using NumPy arrays to store our data.\n",
    "- `scipy` contains useful functions to calculate filter coefficients and carry out filtering. We will be using SciPy filters to clean our EEG data.\n",
    "- `matplotlib` and `seaborn` are used to create plots. We will use them to visually explore and examine our data.\n",
    "- `sklearn` is used for machine learning. We will use this to train our classifier and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                      \n",
    "import matplotlib.pyplot as plt                         \n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import scipy.signal as signal \n",
    "from scipy.io import loadmat\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Pre-processing\n",
    "\n",
    "### Step 1: Design a filter\n",
    "We first create some variables to store our sampling rate (`fs`), the amount of time between samples (`dt`), and various variables for our filter parameters. We'll be using an FIR windowed bandpass filter with 0.1 Hz and 30 Hz cutoffs. Since we'll be filtering on epoched data, we have to utilize a relatively low order/number of taps. We'll utilize the function `filtfilt()` in order to filter the data forwards and backwards (non-casual). This is a zero-phase filter, meaning our ERPs will remain precisely synchronized and our component of interest shouldn't move as a result of filtering.\n",
    "<br><br>\n",
    "Here is the summary of our filter design:\n",
    "- `Filter..................: Hamming Windowed-Sinc (FIR); zero-phase, non-causal`\n",
    "- `Number of coefficients..: 31`\n",
    "- `High pass...............: 0.1 Hz (low cut of our bandpass)`\n",
    "- `Low pass................: 30 Hz (high cut of our bandpass)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our filter variables\n",
    "fs = 240.0                      # Hz; sampling rate\n",
    "dt = 1000. / fs                 # ms; time between samples\n",
    "sdt = np.round(dt).astype(int); # rounded dt so that we can index samples\n",
    "hp = 0.1                        # Hz; our low cut for our bandpass\n",
    "lp = 30.                        # Hz; our high cut for our bandpass\n",
    "num_taps = 31                   # Number of taps/coefficients of FIR filter\n",
    "\n",
    "# Create our filter coefficients\n",
    "# Note: by defining 'fs' we don't divide our windows by the Nyquist\n",
    "# Note: for FIR filters, a is always 1\n",
    "b = signal.firwin(numtaps=num_taps, cutoff=[hp, lp], pass_zero='bandpass', fs=fs)\n",
    "a = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Determine our epoch, baseline, and window of focus\n",
    "For the epoch, we'll plot the activity from 0ms (stimulus onset) to 700 ms (after onset) to ensure we have a waveform similar to the plot provided by the authors. For the baseline, we'll use 0ms to 100 ms after stimulus onset. This baseline window was chosen because every epoch contains information from the previous stimulus due to the small interstimulus interval (ISI). For the window we will select our ERP component from, we'll use 200 ms after the stimulus to 400 ms after the stimulus. This choice is consistent with the theoretical expected onset of the P3b, and minimizes signals from previous and subsequent stimuli. For readibility:\n",
    "- `Epoch...: 0 - 700 ms`\n",
    "- `Baseline: 0 - 100 ms`\n",
    "- `ERP.....: 200 - 400 ms`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ERP-related variables\n",
    "epoch_start = 0    # ms\n",
    "epoch_end = 700    # ms\n",
    "baseline_start = 0 # ms\n",
    "baseline_end = 100 # ms\n",
    "erp_start = 200    # ms\n",
    "erp_end = 400      # ms\n",
    "\n",
    "# Let's translate these from time into index space to save time later\n",
    "e_s = np.round(epoch_start / sdt).astype(int)     # epoch start\n",
    "e_e = np.round(epoch_end / sdt).astype(int)       # epoch end\n",
    "bl_s = np.round(baseline_start / sdt).astype(int) # baseline start\n",
    "bl_e = np.round(baseline_end / sdt).astype(int)   # baseline end\n",
    "erp_s = np.round(erp_start / sdt).astype(int)     # ERP component window start\n",
    "erp_e = np.round(erp_end / sdt).astype(int)       # ERP component window end\n",
    "\n",
    "# We'll also create our channel labels here since the document only provided a figure\n",
    "channels = np.array(['FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6',\n",
    "                     'C5',  'C3',  'C1',  'Cz',  'C2',  'C4',  'C6',\n",
    "                     'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6',\n",
    "                     'Fp1', 'Fpz', 'Fp2', 'AF7', 'AF3', 'AFz', 'AF4', 'AF8',\n",
    "                     'F7',  'F5',  'F3',  'F1',  'Fz',  'F2',  'F4',  'F6', 'F8',\n",
    "                     'FT7', 'FT8', 'T7',  'T8',  'T9',  'T10', 'TP7', 'TP8', \n",
    "                     'P7',  'P5',  'P3',  'P1',  'Pz',  'P2',  'P4',  'P6', 'P8',\n",
    "                     'PO7', 'PO3', 'POz', 'PO4', 'PO8', 'O1',  'Oz',  'O2', 'Iz'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load and epoch our data\n",
    "\n",
    "We are now ready to extract our signals of interest. All of our data is contained in the *data* subfolder within this directory. Inside are 19 ```.mat``` files. These files were all recorded from the same subject performing unique runs of copy spelling. Copy spelling is when a subject is told which words/letters they will be spelling so that we can label their data and train a classifier to be used during free spelling. In this case, as mentioned in the document, our training data have the prefix ***AAS010*** and ***AAS011*** , while our test (unlabeled) data have the prefix ***AAS012***\n",
    "<br><br>\n",
    "Using our previously defined epoch-related and filter-related variables, we will iterate through all of our files, load them in, extract the relevant pieces of information, process our signals, and store them in easily accessed structures. Our training data, ```train_data```, will also be accompanied by ```train_labels``` which will be a binary vector with 0 indicating non-target and 1 indicating a target, and ```train_markers```, which will contain the row/column which was illuminated during the trial. For our testing data, ```test_data``` will contain our epochs of interest, but since we have no labels for this data, it will only be accompanied by ```test_markers```.\n",
    "<br><br>\n",
    "It's important to note that we will be filtering our EEG signals as we append them to their respective structures.\n",
    "<br><br>\n",
    "***Since the code below is written to be maximally readable, it will be relatively (aka very) slow.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store our data. We'll conver them into np.arrays at the end\n",
    "train_data = []\n",
    "train_labels = []\n",
    "train_markers = []\n",
    "test_data = []\n",
    "test_markers = []\n",
    "\n",
    "# Load all data (concatenate train data, keep test separate)\n",
    "pfx = 'AAS0'\n",
    "dir_ = './data/'\n",
    "train = [10, 11]\n",
    "S = [10, 11, 12]\n",
    "R = [x for x in range(1, 9)]\n",
    "for s in S:\n",
    "    for r in R:\n",
    "        #fp = Path(dir_ + pfx + '%dR0%d.mat' % (s, r))\n",
    "        fp = Path(f'{dir_}{pfx}{s}R0{r}.mat')\n",
    "        if fp.exists() and fp.is_file():\n",
    "            # If file exists, load, process, append\n",
    "            print(f'Processing: {fp}')\n",
    "            x = loadmat(fp)\n",
    "\n",
    "            # All we need are the EEG data (signal) and markers (StimulusCode) out of this\n",
    "            data_   = np.array(x['signal'])        # dim (num(time_points) x num(channels))\n",
    "            mrks_   = x['StimulusCode'].flatten(); # dim (num(time_points) x 1)\n",
    "\n",
    "            # Create a stimulus onset array\n",
    "            onsets = np.zeros_like(mrks_)\n",
    "            for i in range(mrks_.shape[0]):\n",
    "                # Roughly equivalent to positive first derivative, but easier to read\n",
    "                if mrks_[i] != 0 and mrks_[i-1] == 0:\n",
    "                    onsets[i] = 1\n",
    "                    \n",
    "            # Determine all indices where onsets occur\n",
    "            onsets_ix = np.where(onsets == 1)[0]\n",
    "        \n",
    "            # Iterate through onsets, epoch, filter, append\n",
    "            for i in range(len(onsets_ix)):\n",
    "                t0 = onsets_ix[i]\n",
    "                \n",
    "                # Epoch and correct DC offset of signal\n",
    "                data = data_[e_s+t0:t0+e_e, :] - np.mean(data_[e_s+t0:t0+e_e, :], 0)\n",
    "                \n",
    "                # Filter the epoch (non-causal, zero-phase filter)\n",
    "                data = signal.filtfilt(b, a, data, axis=0)\n",
    "                \n",
    "                # Now let's baseline correct\n",
    "                data = data - np.mean(data[bl_s+np.abs(e_s):np.abs(e_s)+bl_e, :], 0)\n",
    "                \n",
    "                # Append data to correct locations\n",
    "                if s in train:\n",
    "                    train_data.append(data)\n",
    "                    train_markers.append(mrks_[t0])\n",
    "                    train_labels.append(x['StimulusType'].flatten()[t0]) # target or non-target\n",
    "                else:\n",
    "                    test_data.append(data)\n",
    "                    test_markers.append(mrks_[t0])\n",
    "                    \n",
    "print('Processing completed!')\n",
    "\n",
    "# Convert all of our lists into numpy arrays\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "train_markers = np.array(train_markers)\n",
    "test_data = np.array(test_data)\n",
    "test_markers = np.array(test_markers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our epoched signals, let's divide our ```train_data``` into target and non-target categories for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate target and non-target for plotting\n",
    "tar     = train_data[np.where(train_labels == 1)[0], :, :]\n",
    "non_tar = train_data[np.where(train_labels == 0)[0], :, :]\n",
    "\n",
    "print(f'We have {tar.shape[0]} target trials')\n",
    "print(f'We have {non_tar.shape[0]} non-target trials')\n",
    "\n",
    "# We'll take the average of all trials to create an averaged ERP\n",
    "tar_avg     = np.mean(tar, 0)\n",
    "non_tar_avg = np.mean(non_tar, 0)\n",
    "\n",
    "# Define channel of interest and create an array of time points\n",
    "chan = 'Pz' # let's plot Pz\n",
    "ch = np.where(channels == chan)[0][0]\n",
    "times = np.linspace(epoch_start, epoch_end, train_data.shape[1])\n",
    "\n",
    "# Initialize plot and calculate min and max y value\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 6))\n",
    "min_y = min(np.min(tar_avg), np.min(non_tar_avg))\n",
    "max_y = max(np.max(tar_avg), np.max(non_tar_avg))\n",
    "\n",
    "# Plot x and y axes\n",
    "plt.plot([np.min(times), np.max(times)], [0, 0], color='k');  # x-axis\n",
    "plt.plot([0, 0], [min_y, max_y], color='k');                  # y-axis\n",
    "\n",
    "# Plot our averaged ERPs\n",
    "plt.plot(times, tar_avg[:, ch], 'b', linewidth=4)\n",
    "plt.plot(times, non_tar_avg[:, ch], 'r', linewidth=4)\n",
    "\n",
    "# Highlight the baseline window and window of interest of our ERP\n",
    "baseline = patches.Rectangle([baseline_start, min_y], baseline_end, np.abs(min_y)+max_y, \n",
    "                             color='c', alpha=0.2)\n",
    "erp_win = patches.Rectangle([erp_start, min_y], erp_end-erp_start, np.abs(min_y)+max_y, \n",
    "                             color='lime', alpha=0.2)\n",
    "\n",
    "# Add our baseline and window of interest highlights\n",
    "ax.add_patch(baseline)\n",
    "ax.add_patch(erp_win)\n",
    "\n",
    "# Manually create legends since patches will corrupt default handles\n",
    "legend_ = [patches.Patch(color='b', label = 'Target (oddball)'),\n",
    "           patches.Patch(color='r', label = 'Non-target (standard)')]\n",
    "\n",
    "# Finalize plot and set a high DPI for a crisp, hi-res figure\n",
    "plt.xlabel('Time (msec)');\n",
    "plt.ylabel('Amplitude (A/D Units)');\n",
    "plt.legend(handles=legend_, loc='upper right');\n",
    "plt.title(f'Event Related Potentials at Channel {chan}');\n",
    "fig.set_dpi(216);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Determine how many signals to extract**<br>\n",
    "There is a lot of mutual information in our data, and it would not be productive to export all of it, specifically because it would take more time to train/cross-validate our model. If you're curious, try modifying ```channel``` to examine the ERPs at different locations. You'll find that there are a lot of simillarities between the waveforms of neighboring channels. Neighboring time points are also very similar and we could faithfully recreate our waveform using significantly fewer samples. With this in mind, we can reduce our dimensionality quickly by sub-sampling our data in time, while preseving all 64 channels. One efficient way to sub-sample in a scenario like this is to calculated the ***windowed means*** of our signal of interest.\n",
    "<br><br>\n",
    "<img src=\"assets/img1.png\">\n",
    "<center>A simple illustration of sub-sampling (more generalized than windowed means)</center>\n",
    "<br><br>\n",
    "To calculate our windowed means, we can divide our data into equally sized chunks and calculate the means. In some instances it can be beneficial to have overlap between windows, but for this case we can keep the windows totally seperated. Let's extract 5 points per waveform per channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the windowed means within erp_start and erp_end\n",
    "num_points = 5; # we will divide our window into num_points means\n",
    "\n",
    "# Define a simple windowed means function\n",
    "def wm(x, start, end, num_points):\n",
    "    num_trials = x.shape[0] # assumes first dem is numb observations\n",
    "    w = np.round((start+end)/num_points).astype(int)\n",
    "    y = np.zeros((num_points, x.shape[-1], num_trials)) # assumes num chans as last dimension\n",
    "    for i in range(0, num_points):\n",
    "        s = start + (w * i)\n",
    "        e = end   + (w * i)\n",
    "        y[i, :, :] = np.mean(x[:, s:e, :], 1).T\n",
    "    return y\n",
    "\n",
    "# Combine into a single train variable. Also create labels\n",
    "X_train    = wm(train_data, erp_s, erp_e, num_points)\n",
    "markers_train = np.vstack((train_labels, train_markers)).T\n",
    "y = train_labels\n",
    "\n",
    "# Now let's compute windowed means of our test data\n",
    "X_test = wm(test_data, erp_s, erp_e, num_points)\n",
    "markers_test = test_markers\n",
    "\n",
    "# Let's print out the shapes of our data\n",
    "print(f'X_train shape is: {X_train.shape}')\n",
    "print(f'y shape is......: {y.shape}')\n",
    "print(f'X_test shape is.: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dimensions of ```X_train``` are 5 x 64 x 7561. We have 5 samples per observation (since we 10-times subsample a window of ~50 samples), 64 channels, and 7561 unique observations. Similarly, we have 5580 unique observations for our test data, ```X_test```. We must restructure our data so that they can be passed into our classifier. Luckily, our 5 x 64 features can be flattened, and we can subsequently transpose our matrix so that the first dimension matches our label vector (this is the format ```sklearn``` expects). We will do the same flattening and transposing on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since our X is 3D, we must flatten our data. We will then transpose it for sklearn\n",
    "X_train = X_train.reshape(-1, X_train.shape[-1]).T\n",
    "X_test = X_test.reshape(-1, X_test.shape[-1]).T\n",
    "\n",
    "# Let's print out the new shape\n",
    "print('X_train shape is now: ' + str(X_train.shape))\n",
    "print('X_test  shape is now: ' + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data are correctly formatted, we can train a model! We will first choose our classifier. One standard choice for classifying P300 data is via linear discriminant analysis (LDA) which has been discussed in class. sklearn supports LDA (and its quadratic counterpart QDA) which makes the process rather simple. \n",
    "<br><br>\n",
    "We will be using the least-squares solver, which tends to be more efficient for higher dimensional feature vectors.\n",
    "<br><br>\n",
    "We will shrink the covariance matrix, which is important when we have high-dimensional data with relatively few observations, and we will determine this parameter automatically. This process is called shrinkage.<br>\n",
    "See: https://scikit-learn.org/stable/auto_examples/classification/plot_lda.html#sphx-glr-auto-examples-classification-plot-lda-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our classifier (this may take a while via JupyterHub)\n",
    "clf = LinearDiscriminantAnalysis(solver='lsqr',  shrinkage='auto').fit(X_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do 5-fold cross validation to determine the accuracy of our model<br>\n",
    "***Note: This may take a few minutes to run on slower machines***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do 5-fold cross validation\n",
    "nfolds = 5;\n",
    "score = cross_val_score(clf.fit(X_train, y), X_train, y, cv=nfolds)\n",
    "\n",
    "# We will print out the mean score\n",
    "print(f'Mean classifier accuracy: {np.mean(score)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know that the non-target stimuli are considerably more common *(10/12, 83%)*, we would expect to get around 80% accuracy by just saying everything is a non-target. In order to examine our model more carefully, we will use the receiver operating characteristic curve (ROC) and calculate the area under the curve (AUC) to see how often our classifier is correctly labeling the data. We will also use 5-fold cross validation for this method.<br>\n",
    "https://en.wikipedia.org/wiki/Receiver_operating_characteristic <br>\n",
    "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/ <br>\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from:\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n",
    "cv = StratifiedKFold(n_splits=nfolds)\n",
    "X = X_train\n",
    "tprs = [];                         # true positive rates\n",
    "aucs = [];                         # areas under curve\n",
    "mean_fpr = np.linspace(0, 1, 100); # false positive rates\n",
    "\n",
    "i = 0\n",
    "for train, test in cv.split(X, y):\n",
    "    probas = clf.fit(X[train], y[train]).predict_proba(X[test])\n",
    "    fpr, tpr, th = roc_curve(y[test], probas[:, 1])\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our ROC and calculated the AUC, let's plot the curve and include the AUCs for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from:\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs);\n",
    "\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=f'Mean ROC (AUC = {mean_auc:0.2f} $\\pm$ {std_auc:0.2f})',\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='b', alpha=.2,\n",
    "                 label='$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'ROC of Least-Squares LDA ({nfolds}-fold CV)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the AUCs agree with our 5-fold cross validation above! We should get fairly good accuracy on our testing data if we haven't been overfitting the data!\n",
    "\n",
    "Let's now use our classifier (trained on all of our training data) and use it to predict outputs from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on our dataset \n",
    "conf = clf.decision_function(X_test) # predicted confidence score (log-posterior)\n",
    "pred = clf.predict(X_test)           # predicted label (we won't actually use this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we have trained a model, and used it to make predictions on individual observations (target vs non-target). We will be using the confidence score of each classification to now determine which character the subject was attending to. The larger the score, the more confident the classification was that the observation was a flashed target.\n",
    "<br><br>\n",
    "Let's create a numpy array to mimic the character matrix described in the data description, in order to efficiently determine which character our classifier predicted\n",
    "<img src=\"assets/img2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the translations for the labels:\n",
    "# Columns 0-5 -> Labels 1-6\n",
    "# Rows 0-5 -> Labels 7-12\n",
    "# Let's create this matrix\n",
    "#                         1    2    3    4    5    6\n",
    "char_matrix = np.array([['A', 'B', 'C', 'D', 'E', 'F'], # 7\n",
    "                        ['G', 'H', 'I', 'J', 'K', 'L'], # 8\n",
    "                        ['M', 'N', 'O', 'P', 'Q', 'R'], # 9\n",
    "                        ['S', 'T', 'U', 'V', 'W', 'X'], # 10\n",
    "                        ['Y', 'Z', '1', '2', '3', '4'], # 11\n",
    "                        ['5', '6', '7', '8', '9', '0']])# 12 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that for each character, each row and column was flashed once, for a total of 12 flashes per block and this was repeated 15 times. This means that the first 12x15 data points correspond to the first character, the 2nd 12x15 data point correspond to the second character, etc.\n",
    "<br><br>\n",
    "Let's examine the first character from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some useful variables\n",
    "char_breaks = 15 * 12 # number of stimuli per character\n",
    "c = 1;                # which character we want\n",
    "\n",
    "# Grab our first character's marker, prediction, and confidence\n",
    "char1_mrks = markers_test[char_breaks*(c-1):char_breaks*c]\n",
    "char1_pred = pred[char_breaks*(c-1):char_breaks*c]\n",
    "char1_conf = conf[char_breaks*(c-1):char_breaks*c]\n",
    "\n",
    "# Let's merge these\n",
    "char1 = np.array([char1_mrks, char1_pred, char1_conf]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an array `char1` which contains 180 stimuli, their marker (which row/col), their prediction (0, 1 unused), and the confidence of the prediction (float). Let's sort our `char1` by confidence in order to determine which row and column gives us the character that we believe to be the subject's target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright, now let's sort them by the 3rd column, the confidence\n",
    "# Nice 1-liner provided by https://stackoverflow.com/questions/2828059/sorting-arrays-in-numpy-by-column\n",
    "char1_sort = char1[char1[:,2].argsort()[::-1]]\n",
    "\n",
    "# Let's print out the first 5 results\n",
    "print(char1_sort[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all the flashes, the one with the highest confidence (that it contains a target) is the marker `'6'` which corresponds to the last column.<br>\n",
    "Our second highest confidence is the marker `'7'` which corresponds to the first row.<br> \n",
    "\n",
    "We could simply take the column/row that has the highest single-flash confidence and consider the target character prediction to be the character in that highest confidence row and highest confident column. However, it's possible that those values are outliers. In order to make better use of all the data, we will take the mean of every marker's (row's and column's) confidence, and use the row and column with the highest average confidence to determine our character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will store the average confidence value for every row/column\n",
    "x = np.zeros(shape = (12, 1))\n",
    "for i in range(0, x.shape[0]):\n",
    "    x[i] = np.mean(char1_sort[char1_sort[:, 0] == i + 1, 2])\n",
    "    \n",
    "# Now let's print our character\n",
    "row = np.argmax(x[6:])\n",
    "col = np.argmax(x[0:6])\n",
    "our_char = char_matrix[row, col]\n",
    "print(f'Our character is: {our_char}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's visualize our confidence intervals over each letter by utilizing a heatmap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define useful plotting variables\n",
    "rows = 6\n",
    "columns = 6\n",
    "fontsize = 20\n",
    "\n",
    "# Initialzie figure/axes\n",
    "fig, ax = plt.subplots(ncols=1, nrows=1, constrained_layout=False, figsize=(8, 6), \n",
    "                       facecolor='white', edgecolor='white', dpi=100)\n",
    "\n",
    "# Let's turn x into a matrix where we take each character's avg CI from row/col\n",
    "x_mat = np.zeros((rows, columns))\n",
    "for j in range(0, columns):\n",
    "    for i in range(columns, rows+columns):\n",
    "        x_mat[i-columns, j] = np.mean((x[i][0], x[j][0]))\n",
    "\n",
    "# Plot confidence interval heatmap\n",
    "sns.heatmap(x_mat, cmap=sns.color_palette('vlag', as_cmap=True), ax=ax, \n",
    "            cbar_kws={'label':'Average Row/Column Confidence Score'})\n",
    "\n",
    "# Plot each character (adapted from https://stackoverflow.com/questions/73071524/)\n",
    "for row in range(rows):\n",
    "    for column in range(columns):\n",
    "        # Plot each individual character over the heatmap\n",
    "        plt.text((column+0.5), (row+0.5),\n",
    "                char_matrix[row][column], fontsize=fontsize, ha='center', va='center')\n",
    "\n",
    "# Hide axes and borders\n",
    "ax.spines['bottom'].set_color('white')\n",
    "ax.spines['top'].set_color('white') \n",
    "ax.spines['right'].set_color('white')\n",
    "ax.spines['left'].set_color('white')\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Our classifier predicts that the first character the subject attended to was the letter `'F'` and the heatmap shows that the letter `'F'` has the highest average confidence interval. Let's now use this logic and apply it iteratively to each character in the test data, and print out every character the user attended to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop: Iterate through test data by chunking every 15x12 trials per character\n",
    "#  Nest 1: Calculate mean of character\n",
    "char_breaks = 15*12\n",
    "output_string = f''\n",
    "for c in range(1, int(X_test.shape[0] / char_breaks) + 1):\n",
    "    # Lets break up test into char 'c'\n",
    "    temp_mrks = markers_test[char_breaks * (c - 1):char_breaks * c]\n",
    "    temp_pred = pred[char_breaks * (c - 1):char_breaks * c]\n",
    "    temp_conf = conf[char_breaks * (c - 1):char_breaks * c]\n",
    "\n",
    "    # Let's merge these and sort by highest confidence\n",
    "    temp_char = np.array([temp_mrks, temp_pred, temp_conf]).T\n",
    "    temp_char = temp_char[temp_char[:, 2].argsort()[::-1]]\n",
    "    \n",
    "    # Create our averages\n",
    "    x = np.zeros(shape = (12, 1))\n",
    "    for i in range(0, x.shape[0]):\n",
    "        x[i, 0] = np.mean(temp_char[temp_char[:, 0] == i + 1, 2])\n",
    "        \n",
    "    # Now let's print our character\n",
    "    row = np.argmax(x[6:,  0])\n",
    "    col = np.argmax(x[0:6, 0])\n",
    "    our_char = char_matrix[row, col]\n",
    "    output_string += f'{our_char} '\n",
    "    \n",
    "# Print our result\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a competition dataset, we were obviously not given the labels to our test set. However, since this competition is quite old (2002), the results are public!\n",
    "\n",
    "5 teams got first place in this competition, each with test character 100% accuracy. The added challenge was to minimize the amount of flashes each character was required, but for simplicity we will not do this.\n",
    "\n",
    "Since all of these teams were able to determine the desired characters with 100% accuracy, we can look at one of their write-ups to determine the true labels of the test set ***(click on link below and scroll to the bottom)***.\n",
    "\n",
    "The approach we took for this assignment was influenced by (but not a direct copy of): http://www.bbci.de/competition/ii/results/tax_iib_desc.pdf\n",
    "<br>\n",
    "***\n",
    "***Question 6:*** What percentage of our test characters were correctly classified?\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
